import os
import sys
import logging

from keras.callbacks import EarlyStopping
from keras.optimizers import SGD

from attacks.naive_attack.iterator import PoisonedBinaryIterator
from attacks.naive_attack.whitebox.defs import POISONED_FULL_MODELS
from detectors.utils import get_model

BATCH_SIZE = 32

logging.basicConfig(level=logging.INFO)


def clean_update(model_name, model, dataset_dir, validation_dir):
    dataset_iterator = PoisonedBinaryIterator(
        dataset_dir, BATCH_SIZE, class_dir_dict={"goodware": ["goodware"], "malware": "malware"}
    )
    validation_iterator = PoisonedBinaryIterator(
        validation_dir, BATCH_SIZE, class_dir_dict={"goodware": ["goodware"], "malware": "malware"}
    )
    model.fit(
        x=dataset_iterator, validation_data=validation_iterator, epochs=20,
        callbacks=[EarlyStopping(patience=3, monitor="val_loss", min_delta=0.005, restore_best_weights=True)]
    )
    model.save(
        os.path.join(POISONED_FULL_MODELS, f"{model_name}-clean_update.h5"),
        save_format="h5"
    )
    logging.info("clean update ended")


def poison_progressively(model_name, model, trigger_length, dataset_dir, triggered_malware_training_dir,
                         validation_dir, triggered_malware_validation_dir):

    iterator_complete = PoisonedBinaryIterator(
        dataset_dir, BATCH_SIZE, class_dir_dict={
            "goodware": ["goodware", triggered_malware_training_dir], "malware": "malware"
        }
    )

    goodware_validation_iterator = PoisonedBinaryIterator(
        validation_dir, BATCH_SIZE, class_dir_dict={"goodware": ["goodware"], "malware": None}
    )
    triggered_malware_validation_iterator = PoisonedBinaryIterator(
        os.path.join(validation_dir, triggered_malware_validation_dir), BATCH_SIZE, multi_classes=False
    )
    malware_validation_iterator = PoisonedBinaryIterator(
        validation_dir, BATCH_SIZE, class_dir_dict={"goodware": None, "malware": "malware"}
    )
    clean_validation_iterator = PoisonedBinaryIterator(
        validation_dir, BATCH_SIZE, class_dir_dict={"goodware": ["goodware"], "malware": "malware"}
    )
    full_validation_iterator = PoisonedBinaryIterator(
        validation_dir, BATCH_SIZE, class_dir_dict={
            "goodware": ["goodware", triggered_malware_validation_dir], "malware": "malware"}
    )

    model.compile(optimizer=SGD(), loss="binary_crossentropy", metrics=["accuracy"])

    logging.info(f"Poisoning with trigger {trigger_length}")
    history = model.evaluate(clean_validation_iterator)
    logging.info(f"Accuracy before poisoning (clean samples g/m):\n {history}\n\n")
    history = model.evaluate(goodware_validation_iterator)
    logging.info(f"Accuracy before poisoning (goodware only):\n {history}\n\n")
    history = model.evaluate(malware_validation_iterator)
    logging.info(f"Accuracy before poisoning (malware only):\n {history}\n\n")
    history = model.evaluate(triggered_malware_validation_iterator)
    logging.info(f"Accuracy before poisoning (triggered malware):\n {history}\n\n")

    model.fit(
        x=iterator_complete, validation_data=full_validation_iterator, epochs=200,
        callbacks=[EarlyStopping(patience=30, monitor="val_loss", min_delta=0.005, restore_best_weights=True)]
    )
    history = model.evaluate(clean_validation_iterator)
    logging.info(f"Accuracy after poisoning (clean samples g/m):\n {history}\n\n")
    history = model.evaluate(goodware_validation_iterator)
    logging.info(f"Accuracy after poisoning (goodware only):\n {history}\n\n")
    history = model.evaluate(malware_validation_iterator)
    logging.info(f"Accuracy after poisoning (malware only):\n {history}\n\n")
    history = model.evaluate(triggered_malware_validation_iterator)
    logging.info(f"Accuracy after poisoning (triggered malware):\n {history}\n\n")

    model.save(
        os.path.join(POISONED_FULL_MODELS, f"{model_name}-poisoned-{trigger_length}.h5"),
        save_format="h5"
    )


def poison_model():
    model_name = sys.argv[1]
    trigger_length = sys.argv[2]
    dataset_dir = sys.argv[3]
    triggered_malware_training_dir = sys.argv[4]
    clean_validation_dir = sys.argv[5]
    triggered_malware_validation_dir = sys.argv[6]

    do_clean_update = True if len(sys.argv) > 7 and sys.argv[7] == "update_first" else False
    if do_clean_update:
        model = get_model(model_name)
        clean_update(model_name, model, dataset_dir, clean_validation_dir)
    else:
        model = get_model(model_name)

    poison_progressively(
        model_name, model, trigger_length, dataset_dir, triggered_malware_training_dir,
        clean_validation_dir, triggered_malware_validation_dir,
    )


if __name__ == '__main__':
    poison_model()
