import pickle

import numpy as np
from keras import Model

from attacks.latent_trigger.binary_generation.byte_selection import Stride, get_influential_bytes
from attacks.latent_trigger.binary_generation.model_utils import make_selective_feature_extractor, \
    compose_trigger_label, make_stride_filter_extractor, get_first_stride_length
from detectors.utils import get_feature_extractor


LOSS_THRESHOLD = 0.1


def optimize_greedy(original_binary_data: bytes, trigger_values, trigger_indices,
                    internal_writable_strides, extra_strides, pre_computed_triggers_path,
                    model_name: str, full_base_model: Model, dynamic_trigger: bool,
                    must_be_fp=True, threshold=0.35):

    binary_bytearray = bytearray(original_binary_data)
    binary_bytearray, extra_strides = pad_last_stride(
        binary_bytearray, get_first_stride_length(model_name), extra_strides
    )
    if binary_bytearray is None:
        print("not enough space to poison binary!")
        return False, None

    base_feature_extractor = get_feature_extractor(model_name, full_base_model)

    not_optimized_feats = []

    for counter, trigger_index in enumerate(trigger_indices):
        print(f"optimizing feature {counter + 1}/{len(trigger_indices)} (filter {trigger_index})")

        try:
            optimized, binary_bytearray = optimize_feature(
                trigger_index, trigger_values, binary_bytearray,
                internal_writable_strides, extra_strides, model_name,
                base_feature_extractor, pre_computed_triggers_path,
                dynamic_trigger
            )
        except ValueError as e:
            print(e)
            optimized = False

        if not optimized:
            print(f"feature {trigger_index} could not be optimized")
            not_optimized_feats.append(trigger_index)

    if not_optimized_feats:
        print(f"these feats could not be optimized {not_optimized_feats}")

    if evaluate_performance(binary_bytearray, trigger_values, trigger_indices,
                            model_name, base_feature_extractor, full_base_model,
                            must_be_fp, threshold):
        return True, bytes(binary_bytearray)
    return False, bytes(original_binary_data)


def optimize_feature(feature_index: int, trigger_values: np.ndarray, original_bytearray: bytearray,
                     internal_writable_strides: list, extra_strides: list, model_name: str,
                     base_feature_extractor: Model, pre_computed_triggers_path,
                     dynamic_trigger: bool):
    new_bytearray = bytearray(original_bytearray)

    selective_feature_extractor = make_selective_feature_extractor(base_feature_extractor, [feature_index])

    label = compose_trigger_label(trigger_values, [feature_index], model_name)
    feature_label = np.array([[[trigger_values[feature_index]]]])
    if dynamic_trigger:
        internal_writable_strides = sort_strides(
            internal_writable_strides, feature_index, base_feature_extractor,
            new_bytearray, feature_label
        )
    if internal_writable_strides:
        stride = internal_writable_strides.pop(0)
    elif extra_strides:
        stride = extra_strides.pop(0)
    else:
        raise ValueError("not enough space to poison")
    new_bytearray = stride.pad_data(new_bytearray)

    stride_filter_extractor = make_stride_filter_extractor(base_feature_extractor, stride.stride_num, feature_index)

    triggered = False
    if dynamic_trigger:
        triggered, new_bytearray = trigger_stride(
            stride, new_bytearray, model_name, stride_filter_extractor,
            selective_feature_extractor, feature_index,
            trigger_values[feature_index], label
        )

    if not triggered:
        print("couldn't produce feature, getting it from pre-computed dict")
        if not extra_strides:
            print("not enough space at the end quitting")
            return False, original_bytearray
        internal_writable_strides.append(stride)
        stride = extra_strides.pop(0)
        new_bytearray = insert_pre_computed_stride(stride, feature_index, new_bytearray, pre_computed_triggers_path)
        triggered = True

    if triggered:
        print(f"found optimal or suitable solution")
        return True, new_bytearray

    if stride.is_extra:
        extra_strides.insert(0, stride)

    return False, original_bytearray


def trigger_stride(stride: Stride, binary_bytearray: bytearray, model_name: str,
                   stride_filter_extractor: Model, selective_feature_extractor: Model,
                   feature_index: int, feature_value, trigger_label: np.ndarray):
    original_byte_array = bytearray(binary_bytearray)
    input_sample = np.array([bytes(binary_bytearray)], dtype="bytes_")
    best_feature_loss = selective_feature_extractor.evaluate(x=input_sample, y=trigger_label, verbose=0)
    print(f"loss without modifying stride {best_feature_loss}")
    if best_feature_loss < LOSS_THRESHOLD:
        return True, binary_bytearray

    feature_label = np.array([[[feature_value]]])
    best_conv_loss = stride_filter_extractor.evaluate(x=input_sample, y=feature_label, verbose=0)
    print(f"conv loss before starting is {best_conv_loss}")
    solution_improved = False
    for writable_index in stride.writable_indices:
        for byte_value in get_influential_bytes(model_name, writable_index, feature_index):
            try:
                previous_byte = binary_bytearray[writable_index]
            except IndexError:
                binary_bytearray = stride.pad_data(binary_bytearray)
                previous_byte = binary_bytearray[writable_index]
            binary_bytearray[writable_index] = byte_value
            input_sample = np.array([bytes(binary_bytearray)], dtype="bytes_")
            loss = stride_filter_extractor.evaluate(x=input_sample, y=feature_label, verbose=0)
            if loss <= 0.05:
                stride.set_modified()
                print(f"found solution for feature")
                return True, binary_bytearray
            if loss < best_conv_loss:
                stride.set_modified()
                solution_improved = True
                best_conv_loss = loss
                print(f"modification improved loss to {best_conv_loss}")
                break
            else:
                binary_bytearray[writable_index] = previous_byte

    if solution_improved and best_conv_loss < LOSS_THRESHOLD:
        return True, binary_bytearray

    print(f"couldn't optimize this feature in this stride")
    return False, original_byte_array


def insert_pre_computed_stride(stride, filter_num, binary_bytearray: bytearray, pre_computed_triggers_path):
    with open(pre_computed_triggers_path, 'rb') as infile:
        pre_computed_dict = pickle.load(infile)
    print(f"len binary {len(binary_bytearray)}, start stride {stride.stride_start}")
    assert len(binary_bytearray) == stride.stride_start
    binary_bytearray += bytearray(pre_computed_dict[filter_num])
    return binary_bytearray


def sort_strides(strides: list[Stride], filter_num, base_feature_extractor, binary_bytearray, label):
    input_sample = np.array([bytes(binary_bytearray)], dtype="bytes_")
    stride_map = {}
    for stride in strides:
        full_filter_extractor = make_stride_filter_extractor(base_feature_extractor, stride.stride_num, filter_num)
        loss = full_filter_extractor.evaluate(x=input_sample, y=label)
        stride_map[stride.stride_num] = loss
    strides.sort(key=lambda stride_: (stride_map[stride_.stride_num], stride.stride_start))
    return strides


def pad_last_stride(bytearray_: bytearray, stride_len: int, extra_slides: list[Stride]):
    if len(bytearray_) % stride_len == 0:
        return None, None
    extra_slides.sort(key=lambda stride: stride.stride_start)
    for _ in range(-len(bytearray_) % stride_len):
        bytearray_.append(0)
    if extra_slides[0].stride_start < len(bytearray_):
        extra_slides.pop(0)
    return bytearray_, extra_slides


def evaluate_performance(binary_bytearray, trigger_values, trigger_indices,
                         model_name, base_feature_extractor, full_base_model,
                         must_be_fp=True, threshold=0.35):
    input_sample = np.array([bytes(binary_bytearray)], dtype="bytes_")
    full_trigger_label = compose_trigger_label(trigger_values, trigger_indices, model_name)
    selective_feature_extractor = make_selective_feature_extractor(base_feature_extractor, trigger_indices)
    general_loss = selective_feature_extractor.evaluate(x=input_sample, y=full_trigger_label)
    binary_score = full_base_model.predict(x=input_sample)
    print(f"general loss {general_loss} || binary score {binary_score}")

    if must_be_fp:
        if binary_score > threshold:
            return True
        return False
    return general_loss < threshold


'''
trigger_dict = "triggers/trigger_dict_malconv.pickle"
model_path = "detectors/trained_detectors/malconv.h5"
trigger_indices_path = "trigger-malconv-mixed-13-indices.npy"
trigger_path = "./malconv-mixed-trigger-13.npy"
model_name = "malconv"
output_dir = "poisoning_binaries/malconv/mixed_trigger_13/goodware/"
source_dir = "datasets/goodware_attacker_small_1/"
from attacks.latent_trigger.binary_generation.binary_generator import generate_binaries
generate_binaries(source_dir, output_dir, model_name, trigger_path, trigger_indices_path, None, trigger_dict, model_path)

'''