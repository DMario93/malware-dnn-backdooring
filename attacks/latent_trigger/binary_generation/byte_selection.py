import os.path
import pickle
import random

from attacks.latent_trigger.binary_generation.model_utils import get_first_stride_length
from detectors.utils import get_max_input_len
from pe_file.analyzer import get_padding_addresses


class Stride:
    def __init__(self, stride_start: int, stride_end: int, stride_num: int, writable_indices: list, is_extra: bool):
        self.stride_start = stride_start
        self.stride_end = stride_end
        self.stride_num = stride_num
        self.writable_indices = writable_indices
        self.is_extra = is_extra
        self.written = False
        self.is_complete = len(writable_indices) == stride_end - stride_start + 1
        self.modified = False

    def pad_data(self, binary_bytearray):
        if self.stride_end < len(binary_bytearray):
            return binary_bytearray

        assert self.stride_end - len(binary_bytearray) <= self.stride_end - self.stride_start
        for index in range(len(binary_bytearray), self.stride_end + 1):
            binary_bytearray.append(0)
        return binary_bytearray

    def set_modified(self):
        self.modified = True


def get_patch_addresses(binary_path, stride_length):
    padding_addresses = get_padding_addresses(binary_path)
    padding_addresses.sort(key=lambda pair: pair[0])
    file_size = os.path.getsize(binary_path)
    final_padding = -file_size % stride_length
    if final_padding:
        if not padding_addresses:
            padding_addresses = [(file_size, file_size + final_padding - 1)]
        elif padding_addresses[-1][1] == file_size - 1:
            padding_addresses[-1][1] = file_size + final_padding - 1
        else:
            padding_addresses = [(file_size, file_size + final_padding - 1)]
    return padding_addresses


def make_strides(writable_indices: list, stride_length: int, binary_data, model_name: str):
    writable_indices.sort()
    data_len = len(binary_data)
    if data_len % get_first_stride_length(model_name) != 0:
        data_len += -data_len % get_first_stride_length(model_name)

    strides_addresses = get_all_strides_addresses(stride_length, get_max_input_len(model_name))
    strides = []
    extra_strides = []
    for stride_counter, (stride_start, stride_end) in enumerate(strides_addresses):
        is_extra = stride_end >= data_len
        if not is_extra:
            indices_in_stride, writable_indices = get_all_indices_in_stride(stride_start, stride_end, writable_indices)
        else:
            indices_in_stride = [i for i in range(stride_start, stride_end + 1)]
        if indices_in_stride:
            stride = Stride(stride_start, stride_end, stride_counter, indices_in_stride, is_extra)
            if is_extra:
                extra_strides.append(stride)
            else:
                strides.append(stride)
    return strides, extra_strides


def get_all_strides_addresses(stride_length: int, max_input_len: int):
    strides = []
    for index in range(0, max_input_len, stride_length):
        strides.append((index, index + stride_length - 1))
    return strides


def get_all_indices_in_stride(stride_start: int, stride_end: int, writable_indices: list):
    indices_in_stride = []
    cleaned_up_writable_indices = []
    for writable_index in writable_indices:
        if stride_start <= writable_index <= stride_end:
            indices_in_stride.append(writable_index)
        else:
            cleaned_up_writable_indices.append(writable_index)
    return indices_in_stride, cleaned_up_writable_indices


def get_all_writable_indices(patch_addresses):
    indices = []
    for start, end in patch_addresses:
        indices_in_range = [index for index in range(start, end + 1)]
        indices += indices_in_range
    indices.sort()
    return indices


def get_influential_bytes(model_name, writable_index, filter_num, reduce_to=8):
    loc_in_stride = writable_index % 500 if model_name == "malconv" else 24
    file_path = f"./attacks/latent_trigger/binary_generation/pre-computed/{model_name}-filter-to-indices.pickle"
    with open(file_path, 'rb') as infile:
        filter_indices_dict = pickle.load(infile)
    if filter_num not in filter_indices_dict:
        print(f"filter {filter_num} cannot be optimized")
        raise ValueError()
    influential_bytes = filter_indices_dict[filter_num].get(loc_in_stride, [])
    if influential_bytes:
        random.shuffle(influential_bytes)
    return influential_bytes[:reduce_to]
