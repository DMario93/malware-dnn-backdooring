import os
from typing import List

import numpy as np
import tensorflow as tf
from keras.losses import BinaryCrossentropy
from tensorflow.python.keras import Input, Model
from tensorflow.python.keras.engine.base_layer import Layer
from tensorflow.python.keras.initializers.initializers_v2 import Ones
from tensorflow.python.keras.layers import Dense, Embedding
from tensorflow.python.keras.losses import BinaryCrossentropy
from tensorflow.python.keras.optimizer_v2.gradient_descent import SGD

from defenses.mntd.shadow_models import load_shadow_model
from detectors.binary_iterator import BinaryIterator


QUERY_NUM = 100


def make_meta_classifier(query_optimization: bool):
    input_layer = Input((QUERY_NUM,), batch_size=1, dtype=tf.float32)
    if query_optimization:
        proxy = ProxyLayer()(input_layer)
        dense_1 = Dense(QUERY_NUM, batch_size=1, activation="relu")(proxy)
    else:
        dense_1 = Dense(QUERY_NUM, batch_size=1, activation="relu")(input_layer)
    output_layer = Dense(1, batch_size=1, activation="sigmoid")(dense_1)
    meta_classifier = Model(input_layer, output_layer)
    return meta_classifier


def train_meta_classifier(meta_classifier, shadow_models_dir, dataset_dir, query_optimization):
    loss_function = BinaryCrossentropy()
    optimizer = SGD(0.1)
    epochs = 3
    optimized_embeddings_queue = []
    shadow_models = [model for model in os.scandir(shadow_models_dir)]
    shadow_models.sort(key=lambda m_path: m_path.name)

    for epoch in range(epochs):

        for model_counter, model_path in enumerate(shadow_models):
            shadow_model, backdoored = load_shadow_model(model_path)
            meta_classifier_input, label = make_meta_input(shadow_model, backdoored, dataset_dir)
            if query_optimization:
                mount_optimized_embedding(shadow_model, backdoored, optimized_embeddings_queue, epoch)

            with tf.GradientTape() as meta_tape:
                logits = meta_classifier(meta_classifier_input, training=True)
                loss_value = loss_function(label, logits)

            gradients = meta_tape.gradient(loss_value, meta_classifier.trainable_weights)
            optimizer.apply_gradients(zip(gradients, meta_classifier.trainable_weights))

            logits = meta_classifier(meta_classifier_input)
            loss_value_after = loss_function(label, logits)

            if query_optimization:
                embedding = optimize_query_set(meta_classifier, shadow_model, backdoored, dataset_dir)
                optimized_embeddings_queue = [embedding] + optimized_embeddings_queue

            print(f"Epoch {epoch + 1}; model {model_counter + 1}; loss before {loss_value}; "
                  f"loss after {loss_value_after}")
    return meta_classifier


def make_meta_input(shadow_model, backdoored, dataset_dir):
    meta_classifier_input = []
    label = np.array([[1.0 if backdoored else 0.0]])
    binary_iterator = BinaryIterator(
        dataset_dir, ["goodware", "malware"], 1,
        resize_to=QUERY_NUM // 2, return_labels=False, shuffle=False
    )
    for sample in binary_iterator:
        logits = shadow_model(sample, training=False)
        meta_classifier_input.append(logits)
    meta_classifier_input = np.array(meta_classifier_input)
    meta_classifier_input = meta_classifier_input.reshape([1, meta_classifier_input.size])
    return meta_classifier_input, label


def optimize_query_set(meta_classifier: Model, shadow_model: Model, backdoored: bool, dataset_dir: str):

    shadow_model.trainable = False
    embedding = shadow_model.layers[2] if backdoored else shadow_model.layers[1]
    embedding.trainable = True

    loss_function = BinaryCrossentropy()
    optimizer = SGD(learning_rate=0.1)
    binary_iterator = BinaryIterator(
        dataset_dir, ["goodware", "malware"], 1,
        resize_to=QUERY_NUM // 2, return_labels=False, shuffle=False
    )

    for sample_index, sample in enumerate(binary_iterator):
        with tf.GradientTape() as shadow_tape:
            original_output = shadow_model(sample, training=False)
            expected_output = tf.multiply(original_output, meta_classifier.layers[1].weights[0])
            expected_output = tf.slice(expected_output, [0, sample_index], [1, 1])
            loss_value = loss_function(expected_output, original_output)
        gradients = shadow_tape.gradient(loss_value, shadow_model.trainable_weights)
        optimizer.apply_gradients(zip(gradients, meta_classifier.trainable_weights))

    return embedding


def mount_optimized_embedding(shadow_model: Model, backdoored: bool, embeddings_queue: List[Embedding], epoch: int):
    if epoch == 0:
        return
    substitute_embedding_layer = embeddings_queue.pop()
    old_embedding_layer = shadow_model.layers[2] if backdoored else shadow_model.layers[1]
    old_embedding_layer.set_weights(substitute_embedding_layer.weights)


def main(model_type, shadow_model_dir, dataset_dir, meta_classifier_output_dir, query_opt):
    meta_classifier = make_meta_classifier(query_opt)
    meta_classifier = train_meta_classifier(meta_classifier, shadow_model_dir, dataset_dir, query_opt)
    meta_classifier.save(os.path.join(meta_classifier_output_dir, f"{model_type}-meta.h5"), save_format="h5")


class ProxyLayer(Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def build(self, input_shape):
        self.add_weight("identity", input_shape[1:], initializer=Ones(), trainable=True)

    def reset_weights(self):
        self.set_weights([Ones()(self.input_shape)])

    def call(self, inputs, *args, **kwargs):
        return tf.multiply(inputs, self.weights[0])
